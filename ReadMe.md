# ğŸ“Š Sistema de CDC (Change Data Capture) - Kaggle para AWS S3# ğŸ“Š Sistema de CDC (Change Data Capture) - Kaggle para AWS S3# Sistema de CDC (Change Data Capture) - Kaggle para S3# CDC com dados do Kaggle



[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)

[![AWS S3](https://img.shields.io/badge/AWS-S3-orange.svg)](https://aws.amazon.com/s3/)

[![Pandas](https://img.shields.io/badge/Pandas-2.2.0-green.svg)](https://pandas.pydata.org/)[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)

[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

[![AWS S3](https://img.shields.io/badge/AWS-S3-orange.svg)](https://aws.amazon.com/s3/)

> **Pipeline automatizado de ETL para captura incremental de dados (CDC) com integraÃ§Ã£o Kaggle â†’ AWS S3**

[![Pandas](https://img.shields.io/badge/Pandas-2.2.0-green.svg)](https://pandas.pydata.org/)Sistema completo de **Change Data Capture** que automatiza o download de datasets do Kaggle, processa snapshots em formato Parquet e envia para um data lake no AWS S3.## Sobre

---

[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ¯ Sobre o Projeto



Este projeto implementa um **pipeline completo de Change Data Capture (CDC)** que automatiza o processo de extraÃ§Ã£o, transformaÃ§Ã£o e carga de dados do Kaggle para um Data Lake na AWS S3. O sistema detecta automaticamente mudanÃ§as nos dados (inserÃ§Ãµes, atualizaÃ§Ãµes e deleÃ§Ãµes), converte os arquivos para formato Parquet otimizado e mantÃ©m um histÃ³rico incremental de alteraÃ§Ãµes.

> Pipeline automatizado de ETL para captura incremental de dados (CDC) com integraÃ§Ã£o Kaggle â†’ AWS S3

### ğŸ“ Objetivo de Aprendizado

## ğŸ“‹ CaracterÃ­sticasAutomaÃ§Ã£o para baixar dados do Kaggle e criar arquivos de CDC (Change Data Capture) comparando as duas Ãºltimas versÃµes.

Desenvolvido como projeto de portfÃ³lio para demonstrar competÃªncias em:

- **Engenharia de Dados**: ETL, CDC, Data Lakes## ğŸ¯ Sobre o Projeto

- **Cloud Computing**: AWS S3, Boto3

- **Python**: Pandas, Logging, AutomaÃ§Ã£o

- **Boas PrÃ¡ticas**: CÃ³digo limpo, documentaÃ§Ã£o, tratamento de erros

Este projeto implementa um **pipeline completo de Change Data Capture (CDC)** que automatiza o processo de extraÃ§Ã£o, transformaÃ§Ã£o e carga de dados do Kaggle para um Data Lake na AWS S3. O sistema detecta automaticamente mudanÃ§as nos dados (inserÃ§Ãµes, atualizaÃ§Ãµes e deleÃ§Ãµes), converte os arquivos para formato Parquet otimizado e mantÃ©m um histÃ³rico incremental de alteraÃ§Ãµes.

---

- âœ… Download automatizado de datasets do KaggleOs arquivos de CDC estarÃ£o disponÃ­veis no diretÃ³rio `./data/cdc/` no seguinte formato: `{tabela}_YYYMMDD_HHMMSS.csv`

## âœ¨ Principais Funcionalidades

### ğŸ“ Objetivo de Aprendizado

### ğŸ”„ Change Data Capture (CDC)

- âœ… DetecÃ§Ã£o automÃ¡tica de **inserÃ§Ãµes**, **atualizaÃ§Ãµes** e **deleÃ§Ãµes**- âœ… ConversÃ£o de CSV para Parquet

- âœ… ComparaÃ§Ã£o baseada em Primary Keys e campos de data

- âœ… GeraÃ§Ã£o de snapshots incrementais em ParquetDesenvolvido como projeto de portfÃ³lio para demonstrar competÃªncias em:

- âœ… Timestamps automÃ¡ticos para rastreabilidade

- **Engenharia de Dados**: ETL, CDC, Data Lakes- âœ… Upload de full-load para S3## Setup

### ğŸš€ Pipeline Automatizado

- âœ… Download automÃ¡tico de datasets do Kaggle via API- **Cloud Computing**: AWS S3, Boto3

- âœ… ConversÃ£o otimizada de CSV para Parquet (reduÃ§Ã£o de ~70% no tamanho)

- âœ… Upload incremental para AWS S3 com estrutura organizada- **Python**: Pandas, Logging, AutomaÃ§Ã£o- âœ… DetecÃ§Ã£o de CDC (inserÃ§Ãµes, atualizaÃ§Ãµes e deleÃ§Ãµes)

- âœ… Limpeza automÃ¡tica de arquivos antigos (retention configurÃ¡vel)

- **Boas PrÃ¡ticas**: CÃ³digo limpo, documentaÃ§Ã£o, tratamento de erros

### ğŸ“Š GestÃ£o de Dados

- âœ… Suporte a mÃºltiplas tabelas configurÃ¡veis via JSON- âœ… Logging estruturado com timestamps### Instalando dependÃªncias

- âœ… Full-load inicial + cargas incrementais (CDC)

- âœ… Versionamento automÃ¡tico com timestamps---

- âœ… Logging estruturado com nÃ­veis de severidade

- âœ… Tratamento robusto de erros

### âš™ï¸ ConfiguraÃ§Ã£o FlexÃ­vel

- âœ… ConfiguraÃ§Ã£o via JSON (`config.json`)## âœ¨ Principais Funcionalidades

- âœ… VariÃ¡veis de ambiente para credenciais (`.env`)

- âœ… Agendamento automÃ¡tico de execuÃ§Ãµes- âœ… Suporte a execuÃ§Ã£o Ãºnica ou agendadaSugirimos criar um ambiente virtual apropriado para execuÃ§Ã£o do programa. ApÃ³s isso, pode instalar as depedÃªncias usando `pip`:

- âœ… Tratamento robusto de erros e retry logic

### ğŸ”„ Change Data Capture (CDC)

---

- âœ… DetecÃ§Ã£o automÃ¡tica de **inserÃ§Ãµes**, **atualizaÃ§Ãµes** e **deleÃ§Ãµes**- âœ… ConfiguraÃ§Ã£o via JSON e variÃ¡veis de ambiente

## ğŸ—ï¸ Arquitetura do Sistema

- âœ… ComparaÃ§Ã£o baseada em Primary Keys e campos de data

```

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”- âœ… GeraÃ§Ã£o de snapshots incrementais em Parquet```bash

â”‚   Kaggle    â”‚â”€â”€1â”€â”€â–¶â”‚  ETL Python  â”‚â”€â”€2â”€â”€â–¶â”‚   AWS S3    â”‚

â”‚   Dataset   â”‚      â”‚   Pipeline   â”‚      â”‚ Data Lake   â”‚- âœ… Timestamps automÃ¡ticos para rastreabilidade

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                            â”‚## ğŸ› ï¸ Requisitospip install -r requirements.txt

                            â”‚ 3. CDC Detection

                            â–¼### ğŸš€ Pipeline Automatizado

                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

                     â”‚  Local Cache â”‚- âœ… Download automÃ¡tico de datasets do Kaggle via API```

                     â”‚ (Last/Actual)â”‚

                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜- âœ… ConversÃ£o otimizada de CSV para Parquet (reduÃ§Ã£o de ~70% no tamanho)

```

- âœ… Upload incremental para AWS S3 com estrutura organizada- Python 3.8+

### Fluxo de Dados

- âœ… Limpeza automÃ¡tica de arquivos antigos (configurable retention)

1. **ExtraÃ§Ã£o**: Download do dataset via Kaggle API

2. **TransformaÃ§Ã£o**: ConversÃ£o CSV â†’ Parquet + CDC Detection- Conta Kaggle com credenciais de APIPara garantir que a execuÃ§Ã£o funciona corretamente, as seguintes variÃ¡veis ambientes devem estar configuradas:

3. **Carga**: Upload para S3 com estrutura particionada

4. **HistÃ³rico**: ManutenÃ§Ã£o de snapshots locais e remotos### ğŸ“Š GestÃ£o de Dados



---- âœ… Suporte a mÃºltiplas tabelas configurÃ¡veis via JSON- Credenciais AWS com permissÃµes no S3



## ğŸ› ï¸ Tecnologias Utilizadas- âœ… Full-load inicial + cargas incrementais (CDC)



| Categoria | Tecnologias |- âœ… Versionamento automÃ¡tico com timestamps- Bucket S3 criado```bash

|-----------|-------------|

| **Linguagem** | Python 3.8+ |- âœ… Logging estruturado com nÃ­veis de severidade

| **Data Processing** | Pandas, PyArrow (Parquet) |

| **Cloud** | AWS S3, Boto3 |KAGGLE_USERNAME=

| **APIs** | Kaggle API |

| **AutomaÃ§Ã£o** | Schedule, Windows Task Scheduler |### âš™ï¸ ConfiguraÃ§Ã£o FlexÃ­vel

| **Ambiente** | python-dotenv, logging |

- âœ… ConfiguraÃ§Ã£o via JSON (`config.json`)## ğŸ“¦ InstalaÃ§Ã£oKAGGLE_KEY=

---

- âœ… VariÃ¡veis de ambiente para credenciais (`.env`)

## ğŸ“‹ PrÃ©-requisitos

- âœ… Agendamento automÃ¡tico de execuÃ§Ãµes```

- Python 3.8 ou superior

- Conta Kaggle (com API Token)- âœ… Tratamento robusto de erros e retry logic

- Conta AWS (com acesso ao S3)

- Git1. Clone o repositÃ³rio:



------



## ğŸš€ InstalaÃ§Ã£o RÃ¡pida```bashCaso tenha interesse em alterar a periodicidade de atualizaÃ§Ã£o da carga, altere o valor da variÃ¡vel `timer` em `config.json`.



### 1. Clone o RepositÃ³rio## ğŸ—ï¸ Arquitetura do Sistema



```bashgit clone https://github.com/TeoMeWhy/cdc-kaggle.git

git clone https://github.com/TeoMeWhy/cdc-kaggle.git

cd cdc-kaggle```

```

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”cd cdc-kaggle## ExecuÃ§Ã£o

### 2. Instale as DependÃªncias

â”‚   Kaggle    â”‚â”€â”€1â”€â”€â–¶â”‚  ETL Python  â”‚â”€â”€2â”€â”€â–¶â”‚   AWS S3    â”‚

```bash

# Crie um ambiente virtual (recomendado)â”‚   Dataset   â”‚      â”‚   Pipeline   â”‚      â”‚ Data Lake   â”‚```

python -m venv .venv

.venv\Scripts\activate  # Windowsâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



# Instale as dependÃªncias                            â”‚```bash

pip install -r requirements.txt

```                            â”‚ 3. CDC Detection



### 3. Configure as Credenciais                            â–¼2. Instale as dependÃªncias:python main.py



Crie um arquivo `.env` na raiz do projeto:                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”



```env                     â”‚  Local Cache â”‚```bash```

# Kaggle API

KAGGLE_USERNAME=seu_usuario                     â”‚ (Last/Actual)â”‚pip install -r requirements.txt

KAGGLE_KEY=sua_chave_api

                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜```

# AWS Credentials

AWS_ACCESS_KEY_ID=sua_access_key```

AWS_SECRET_ACCESS_KEY=sua_secret_key

AWS_REGION=us-east-13. Configure as credenciais no arquivo `.env`:

```

### Fluxo de Dados```env

### 4. Configure o Pipeline

# Credenciais Kaggle

Edite o arquivo `config.json`:

1. **ExtraÃ§Ã£o**: Download do dataset via Kaggle APIKAGGLE_USERNAME=seu_usuario

```json

{2. **TransformaÃ§Ã£o**: ConversÃ£o CSV â†’ Parquet + CDC DetectionKAGGLE_KEY=sua_chave_api

    "dataset_name": "teocalvo/teomewhy-loyalty-system",

    "aws": {3. **Carga**: Upload para S3 com estrutura particionada

        "bucket": "seu-bucket-s3",

        "prefix": "seu-prefixo",4. **HistÃ³rico**: ManutenÃ§Ã£o de snapshots locais e remotos# Credenciais AWS

        "region": "us-east-1"

    },AWS_ACCESS_KEY_ID=sua_access_key

    "tables": [

        {### Estrutura de DiretÃ³riosAWS_SECRET_ACCESS_KEY=sua_secret_key

            "name": "clientes",

            "pk": "idCliente",AWS_REGION=us-east-1

            "date_field": "DtAtualizacao",

            "sep": ";"``````

        }

    ]cdc-kaggle/

}

```â”œâ”€â”€ data/4. Configure o arquivo `config.json` com suas preferÃªncias:



### 5. Execute o Pipelineâ”‚   â”œâ”€â”€ actual/         # Dados atuais baixados```json



```bashâ”‚   â”œâ”€â”€ last/           # Snapshot anterior (para CDC){

python main.py

```â”‚   â””â”€â”€ cdc/            # Arquivos CDC gerados    "dataset_name": "teocalvo/teomewhy-loyalty-system",



**Para instruÃ§Ãµes detalhadas de instalaÃ§Ã£o e configuraÃ§Ã£o, consulte o [Guia de Setup](docs/SETUP.md).**â”œâ”€â”€ docs/               # DocumentaÃ§Ã£o adicional    "aws": {



---â”œâ”€â”€ main.py             # Pipeline principal        "bucket": "meudatalake-raw",



## ğŸ’¡ Exemplos de Usoâ”œâ”€â”€ config.json         # ConfiguraÃ§Ãµes do pipeline        "prefix": "upcell",



### Estrutura CDC Geradaâ”œâ”€â”€ requirements.txt    # DependÃªncias Python        "region": "us-east-1"



```â”œâ”€â”€ start_pipeline.bat  # Script de inicializaÃ§Ã£o    },

data/cdc/

â”œâ”€â”€ clientes_20251004_095645.parquetâ””â”€â”€ .env               # Credenciais (nÃ£o versionado)    "timer": {

â”œâ”€â”€ produtos_20251004_095646.parquet

â””â”€â”€ transacoes_20251004_095647.parquet```        "unit": "hours",

```

        "value": 6

### Estrutura no S3

---    },

```

s3://seu-bucket/    "tables": [...]

â””â”€â”€ seu-prefixo/

    â”œâ”€â”€ full-load/## ğŸ› ï¸ Tecnologias Utilizadas}

    â”‚   â”œâ”€â”€ clientes.parquet

    â”‚   â”œâ”€â”€ produtos.parquet```

    â”‚   â””â”€â”€ transacoes.parquet

    â””â”€â”€ cdc/| Categoria | Tecnologias |

        â”œâ”€â”€ clientes_20251004_095645.parquet

        â”œâ”€â”€ produtos_20251004_095646.parquet|-----------|-------------|## ğŸš€ Uso

        â””â”€â”€ transacoes_20251004_095647.parquet

```| **Linguagem** | Python 3.8+ |



### Logs do Sistema| **Data Processing** | Pandas, PyArrow (Parquet) |### Modo Agendado (Loop ContÃ­nuo)



```| **Cloud** | AWS S3, Boto3 |

2025-10-04 09:56:43 - INFO - Iniciando pipeline CDC

2025-10-04 09:56:45 - INFO - Dataset baixado com sucesso| **APIs** | Kaggle API |Executa o pipeline continuamente com o intervalo configurado:

2025-10-04 09:56:47 - INFO - CDC detectado: 15 inserÃ§Ãµes, 3 atualizaÃ§Ãµes, 0 deleÃ§Ãµes

2025-10-04 09:56:50 - INFO - Upload para S3 concluÃ­do| **AutomaÃ§Ã£o** | Schedule, Windows Task Scheduler |

```

| **Ambiente** | python-dotenv, logging |```bash

---

python main.py

## ğŸ“ Conceitos Implementados

---```

### Change Data Capture (CDC)



O sistema implementa CDC atravÃ©s de:

## ğŸ“‹ PrÃ©-requisitos### Modo ExecuÃ§Ã£o Ãšnica

1. **Snapshot Comparison**: Compara estado atual vs. anterior

2. **Primary Key Tracking**: Identifica registros Ãºnicos via PK

3. **Timestamp Detection**: Detecta atualizaÃ§Ãµes via campos de data

4. **Operation Types**: Classifica mudanÃ§as como INSERT/UPDATE/DELETE### SoftwareExecuta o pipeline apenas uma vez:



### OtimizaÃ§Ãµes- Python 3.8 ou superior



- **Formato Parquet**: Reduz tamanho em ~70% vs. CSV- Conta Kaggle (com API Token)```bash

- **CompressÃ£o Snappy**: BalanÃ§o entre velocidade e tamanho

- **Batch Processing**: Processa mÃºltiplas tabelas em paralelo- Conta AWS (com acesso ao S3)python main.py --once

- **Retry Logic**: Reexecuta operaÃ§Ãµes em caso de falha

- Git```

---



## ğŸ“– DocumentaÃ§Ã£o Completa

### Credenciais NecessÃ¡rias### Pular Download (Para Testes)

- ğŸ“˜ [**Guia de Setup**](docs/SETUP.md) - InstalaÃ§Ã£o e configuraÃ§Ã£o detalhada

- ğŸ—ï¸ [**Arquitetura**](docs/ARCHITECTURE.md) - Detalhes tÃ©cnicos e fluxo de dados

- â° [**Agendamento**](docs/AGENDAMENTO.md) - Como configurar execuÃ§Ãµes automÃ¡ticas

**Kaggle API**:Executa o pipeline usando dados jÃ¡ baixados localmente:

---

1. Acesse https://www.kaggle.com/settings

## ğŸ“ Estrutura do Projeto

2. Clique em "Create New API Token"```bash

```

cdc-kaggle/3. Baixe o arquivo `kaggle.json`python main.py --once --skip-download

â”œâ”€â”€ data/

â”‚   â”œâ”€â”€ actual/         # Dados atuais baixados```

â”‚   â”œâ”€â”€ last/           # Snapshot anterior (para CDC)

â”‚   â””â”€â”€ cdc/            # Arquivos CDC gerados**AWS S3**:

â”œâ”€â”€ docs/               # DocumentaÃ§Ã£o adicional

â”‚   â”œâ”€â”€ SETUP.md1. Acesse AWS IAM Console### Usar Arquivo de ConfiguraÃ§Ã£o Customizado

â”‚   â”œâ”€â”€ ARCHITECTURE.md

â”‚   â””â”€â”€ AGENDAMENTO.md2. Crie usuÃ¡rio com permissÃµes S3

â”œâ”€â”€ main.py             # Pipeline principal

â”œâ”€â”€ config.json         # ConfiguraÃ§Ãµes3. Gere Access Key ID e Secret Access Key```bash

â”œâ”€â”€ requirements.txt    # DependÃªncias

â””â”€â”€ .env               # Credenciais (nÃ£o versionado)python main.py --config minha_config.json

```

---```

---



## ğŸ“ˆ Melhorias Futuras

## ğŸš€ Como Usar## ğŸ“ Estrutura de DiretÃ³rios

- [ ] IntegraÃ§Ã£o com Apache Airflow para orquestraÃ§Ã£o

- [ ] Suporte a Delta Lake para ACID transactions

- [ ] Dashboard de monitoramento com Grafana

- [ ] Testes unitÃ¡rios e de integraÃ§Ã£o### 1. Clone o RepositÃ³rio```

- [ ] CI/CD com GitHub Actions

- [ ] Suporte a mÃºltiplas fontes de dadoscdc-kaggle/

- [ ] NotificaÃ§Ãµes via SNS/Email

- [ ] MÃ©tricas de qualidade de dados```bashâ”œâ”€â”€ main.py              # Script principal



---git clone https://github.com/TeoMeWhy/cdc-kaggle.gitâ”œâ”€â”€ config.json          # ConfiguraÃ§Ãµes



## ğŸ‘¤ Autorcd cdc-kaggleâ”œâ”€â”€ .env                 # Credenciais (nÃ£o commitar!)



**Tiago Mendes (TeoMeWhy)**```â”œâ”€â”€ requirements.txt     # DependÃªncias Python



- ğŸ’¼ LinkedIn: [linkedin.com/in/seu-perfil](https://linkedin.com/in/seu-perfil)â”œâ”€â”€ cdc_pipeline.log     # Log de execuÃ§Ã£o

- ğŸ™ GitHub: [@TeoMeWhy](https://github.com/TeoMeWhy)

- ğŸ“§ Email: contato@exemplo.com### 2. Instale as DependÃªnciasâ””â”€â”€ data/



---    â”œâ”€â”€ actual/          # Snapshot atual (apÃ³s download)



## ğŸ“„ LicenÃ§a```bash    â”œâ”€â”€ last/            # Snapshot anterior (referÃªncia)



Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.# Crie um ambiente virtual (recomendado)    â””â”€â”€ cdc/             # Arquivos CDC gerados



---python -m venv .venv```



## ğŸ™ Agradecimentos.venv\Scripts\activate  # Windows



- Comunidade Kaggle pelos datasets pÃºblicos## ğŸ—‚ï¸ Estrutura no S3

- AWS Free Tier para testes

- Comunidade Python pela excelente documentaÃ§Ã£o# Instale as dependÃªncias



---pip install -r requirements.txtOs arquivos sÃ£o organizados da seguinte forma:



<div align="center">```



**â­ Se este projeto foi Ãºtil para vocÃª, considere dar uma estrela!**```



**ğŸ“§ Aberto a oportunidades de trabalho em Engenharia de Dados**### 3. Configure as Credenciaiss3://meudatalake-raw/upcell/



</div>â”œâ”€â”€ full-load/


Crie um arquivo `.env` na raiz do projeto:â”‚   â”œâ”€â”€ clientes/

â”‚   â”‚   â””â”€â”€ clientes.parquet

```envâ”‚   â”œâ”€â”€ produtos/

# Kaggle APIâ”‚   â”‚   â””â”€â”€ produtos.parquet

KAGGLE_USERNAME=seu_usuarioâ”‚   â”œâ”€â”€ transacoes/

KAGGLE_KEY=sua_chave_apiâ”‚   â”‚   â””â”€â”€ transacoes.parquet

â”‚   â””â”€â”€ transacao_produto/

# AWS Credentialsâ”‚       â””â”€â”€ transacao_produto.parquet

AWS_ACCESS_KEY_ID=sua_access_keyâ””â”€â”€ cdc/

AWS_SECRET_ACCESS_KEY=sua_secret_key    â”œâ”€â”€ clientes/

AWS_REGION=us-east-1    â”‚   â””â”€â”€ clientes_20251002_143025.parquet

```    â”œâ”€â”€ produtos/

    â”‚   â””â”€â”€ produtos_20251002_143025.parquet

### 4. Configure o Pipeline    â”œâ”€â”€ transacoes/

    â”‚   â””â”€â”€ transacoes_20251002_143025.parquet

Edite o arquivo `config.json`:    â””â”€â”€ transacao_produto/

        â””â”€â”€ transacao_produto_20251002_143025.parquet

```json```

{

    "dataset_name": "teocalvo/teomewhy-loyalty-system",## ğŸ”„ Fluxo do Pipeline

    "aws": {

        "bucket": "seu-bucket-s3",1. **Download**: Baixa o dataset do Kaggle e descompacta em `./data/actual/`

        "prefix": "seu-prefixo",2. **Full-load**: 

        "region": "us-east-1"   - LÃª arquivos CSV

    },   - Converte para Parquet

    "timer": {   - Faz upload para `s3://bucket/prefix/full-load/`

        "unit": "hours",3. **CDC**:

        "value": 6   - Compara snapshot atual com anterior

    },   - Identifica inserÃ§Ãµes (op='I'), atualizaÃ§Ãµes (op='U') e deleÃ§Ãµes (op='D')

    "tables": [   - Salva em Parquet com timestamp

        {   - Faz upload para `s3://bucket/prefix/cdc/`

            "name": "clientes",4. **PÃ³s-processamento**: Move arquivos de `actual/` para `last/` para prÃ³ximo ciclo

            "pk": "idCliente",

            "date_field": "DtAtualizacao",## ğŸ“Š OperaÃ§Ãµes de CDC

            "sep": ";"

        }| OperaÃ§Ã£o | CÃ³digo | DescriÃ§Ã£o |

    ]|----------|--------|-----------|

}| InserÃ§Ã£o | `I` | Registros presentes no snapshot atual mas nÃ£o no anterior |

```| AtualizaÃ§Ã£o | `U` | Registros com PK igual e campo de data maior no snapshot atual |

| DeleÃ§Ã£o | `D` | Registros presentes no snapshot anterior mas nÃ£o no atual |

### 5. Execute o Pipeline

## ğŸ”§ ConfiguraÃ§Ã£o das Tabelas

```bash

# ExecuÃ§Ã£o ÃºnicaNo `config.json`, configure cada tabela com:

python main.py

- `name`: Nome da tabela (e do arquivo CSV)

# Ou use o script de inicializaÃ§Ã£o- `sep`: Separador usado no CSV (ex: `;`, `,`)

start_pipeline.bat- `pk`: Nome da coluna de chave primÃ¡ria

```- `date_field`: Campo usado para detectar atualizaÃ§Ãµes



### 6. (Opcional) Configure AgendamentoExemplo:



Veja a documentaÃ§Ã£o em [`docs/AGENDAMENTO.md`](docs/AGENDAMENTO.md) para configurar execuÃ§Ãµes automÃ¡ticas no Windows Task Scheduler.```json

{

---    "sep": ";",

    "name": "clientes",

## ğŸ“Š Exemplos de Uso    "date_field": "DtAtualizacao",

    "pk": "IdCliente"

### Estrutura CDC Gerada}

```

```

data/cdc/## ğŸ“ Logs

â”œâ”€â”€ clientes_20251004_095645.parquet

â”œâ”€â”€ produtos_20251004_095646.parquetOs logs sÃ£o salvos em dois locais:

â””â”€â”€ transacoes_20251004_095647.parquet- **Console** (stdout): Logs em tempo real

```- **Arquivo** (`cdc_pipeline.log`): HistÃ³rico completo com encoding UTF-8



### Estrutura no S3NÃ­veis de log:

- `INFO`: OperaÃ§Ãµes principais

```- `WARNING`: Avisos (ex: arquivo nÃ£o encontrado)

s3://seu-bucket/- `ERROR`: Erros com traceback completo

â””â”€â”€ seu-prefixo/- `DEBUG`: InformaÃ§Ãµes detalhadas (ativÃ¡vel alterando `logging.INFO` para `logging.DEBUG`)

    â”œâ”€â”€ full-load/

    â”‚   â”œâ”€â”€ clientes.parquet## ğŸ›¡ï¸ Tratamento de Erros

    â”‚   â”œâ”€â”€ produtos.parquet

    â”‚   â””â”€â”€ transacoes.parquetO sistema possui tratamento robusto de erros:

    â””â”€â”€ cdc/

        â”œâ”€â”€ clientes_20251004_095645.parquet- **Download falha**: Pipeline interrompido, erro registrado

        â”œâ”€â”€ produtos_20251004_095646.parquet- **Upload S3 falha**: Erro registrado, continua com prÃ³ximas tabelas

        â””â”€â”€ transacoes_20251004_095647.parquet- **Snapshot anterior ausente**: Considera todas as linhas como inserÃ§Ãµes

```- **Erro em iteraÃ§Ã£o agendada**: Aguarda prÃ³ximo ciclo automaticamente



### Logs do Sistema## ğŸ” SeguranÃ§a



```âš ï¸ **IMPORTANTE**: Nunca commite o arquivo `.env` com credenciais!

2025-10-04 09:56:43 - INFO - Iniciando pipeline CDC

2025-10-04 09:56:45 - INFO - Dataset baixado com sucessoAdicione ao `.gitignore`:

2025-10-04 09:56:47 - INFO - CDC detectado: 15 inserÃ§Ãµes, 3 atualizaÃ§Ãµes, 0 deleÃ§Ãµes```

2025-10-04 09:56:50 - INFO - Upload para S3 concluÃ­do.env

```*.log

data/

---```



## ğŸ“ Conceitos Implementados## ğŸ“– Exemplos de Uso



### Change Data Capture (CDC)### Executar uma vez e verificar logs

```bash

O sistema implementa CDC atravÃ©s de:python main.py --once

tail -f cdc_pipeline.log

1. **Snapshot Comparison**: Compara estado atual vs. anterior```

2. **Primary Key Tracking**: Identifica registros Ãºnicos via PK

3. **Timestamp Detection**: Detecta atualizaÃ§Ãµes via campos de data### Testar com dados locais

4. **Operation Types**: Classifica mudanÃ§as como INSERT/UPDATE/DELETE```bash

python main.py --once --skip-download

### OtimizaÃ§Ãµes```



- **Formato Parquet**: Reduz tamanho em ~70% vs. CSV### Executar em background (Linux/Mac)

- **CompressÃ£o Snappy**: BalanÃ§o entre velocidade e tamanho```bash

- **Batch Processing**: Processa mÃºltiplas tabelas em paralelonohup python main.py > output.log 2>&1 &

- **Retry Logic**: Reexecuta operaÃ§Ãµes em caso de falha```



---### Executar em background (Windows PowerShell)

```powershell

## ğŸ“ˆ Melhorias FuturasStart-Process python -ArgumentList "main.py" -WindowStyle Hidden

```

- [ ] IntegraÃ§Ã£o com Apache Airflow para orquestraÃ§Ã£o

- [ ] Suporte a Delta Lake para ACID transactions## ğŸ¤ Contribuindo

- [ ] Dashboard de monitoramento com Grafana

- [ ] Testes unitÃ¡rios e de integraÃ§Ã£oContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para:

- [ ] CI/CD com GitHub Actions

- [ ] Suporte a mÃºltiplas fontes de dados1. Fazer fork do projeto

- [ ] NotificaÃ§Ãµes via SNS/Email2. Criar uma branch para sua feature (`git checkout -b feature/MinhaFeature`)

- [ ] MÃ©tricas de qualidade de dados3. Commit suas mudanÃ§as (`git commit -m 'Adiciona MinhaFeature'`)

4. Push para a branch (`git push origin feature/MinhaFeature`)

---5. Abrir um Pull Request



## ğŸ“– DocumentaÃ§Ã£o Adicional## ğŸ“„ LicenÃ§a



- [**InstruÃ§Ãµes de Agendamento**](docs/AGENDAMENTO.md) - Como configurar execuÃ§Ãµes automÃ¡ticasEste projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.



---## ğŸ‘¨â€ğŸ’» Autor



## ğŸ‘¤ Autor**TeoMeWhy**

- GitHub: [@TeoMeWhy](https://github.com/TeoMeWhy)

**Tiago (TeoMeWhy)**- Kaggle: [teocalvo](https://www.kaggle.com/teocalvo)



- GitHub: [@TeoMeWhy](https://github.com/TeoMeWhy)## ğŸ™ Agradecimentos

- LinkedIn: [Seu LinkedIn](https://linkedin.com/in/seu-perfil)

- Kaggle pela API de datasets

---- AWS pela infraestrutura S3

- Comunidade Python por bibliotecas incrÃ­veis

## ğŸ“„ LicenÃ§a

---

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

**Desenvolvido com â¤ï¸ e GitHub Copilot**

---

## ğŸ™ Agradecimentos

- Comunidade Kaggle pelos datasets
- AWS Free Tier para testes
- Comunidade Python pela excelente documentaÃ§Ã£o

---

<div align="center">

**â­ Se este projeto foi Ãºtil, considere dar uma estrela!**

</div>
