# Pipeline de IngestÃ£o CDC - Kaggle para AWS S3

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![AWS S3](https://img.shields.io/badge/AWS-S3-orange.svg)](https://aws.amazon.com/s3/)
[![Pandas](https://img.shields.io/badge/Pandas-2.2.0-green.svg)](https://pandas.pydata.org/)

> **Parte 1/2:** Pipeline de ingestÃ£o automatizada com Change Data Capture (CDC) para camada RAW de um Data Lake

---

## ğŸ“– Contexto do Projeto

Este projeto Ã© a **primeira etapa** de uma arquitetura completa de Data Lake/Lakehouse. Aqui, focamos na **ingestÃ£o e geraÃ§Ã£o de dados CDC** que alimentam a camada RAW do data lake.

### ğŸ”— Jornada Completa do Dado

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PARTE 1 (Este Projeto) - IngestÃ£o RAW                       â”‚
â”‚ Kaggle â†’ Python CDC â†’ AWS S3 (Parquet)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PARTE 2 - Data Lake/Lakehouse                               â”‚
â”‚ RAW â†’ BRONZE (Delta + Streaming) â†’ SILVER (CDF) â†’ GOLD      â”‚
â”‚ ğŸ”— github.com/tmarsbr/meu-lago-mago                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Parte 2** ([Meu Lago Mago](https://github.com/tmarsbr/meu-lago-mago)): ConstrÃ³i um Data Lake/Lakehouse completo consumindo os dados gerados aqui, utilizando:
- **BRONZE**: Streaming com UPSERT em Delta Lake
- **SILVER**: Change Data Feed (CDF) para transformaÃ§Ãµes
- **GOLD**: Cubos OLAP para dashboards e anÃ¡lises

---

## ğŸ¯ Objetivo

Automatizar a **extraÃ§Ã£o incremental de dados** (Full-Load + CDC) do Kaggle para um bucket S3, gerando arquivos Parquet otimizados que servirÃ£o como fonte para pipelines de dados downstream.

### O Problema Resolvido

Em ambientes de produÃ§Ã£o, dados transacionais mudam constantemente. Reprocessar datasets completos (full-load) a cada execuÃ§Ã£o Ã© ineficiente. Este pipeline:

1. **Detecta mudanÃ§as** (inserÃ§Ãµes, atualizaÃ§Ãµes, deleÃ§Ãµes)
2. **Gera arquivos CDC incrementais** em Parquet
3. **Armazena no S3** com estrutura organizada
4. **Reduz custos** (~70% menos armazenamento vs. CSV)

---

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Download    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Transform    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kaggle  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Pipeline â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  AWS S3  â”‚
â”‚ Dataset  â”‚     (API)      â”‚  (Python) â”‚   (Parquet)     â”‚   RAW    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â”‚ CDC Detection
                                  â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚ Snapshot      â”‚
                          â”‚ Comparison    â”‚
                          â”‚ (Last/Actual) â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Dados

1. **Extract**: Download dataset via Kaggle API
2. **Transform**: ConversÃ£o CSV â†’ Parquet + DetecÃ§Ã£o CDC
3. **Load**: Upload para S3 (full-load + CDC incremental)
4. **Repeat**: Agendamento automÃ¡tico para execuÃ§Ãµes periÃ³dicas

---

## ğŸ› ï¸ Stack TecnolÃ³gica

| Componente | Tecnologia | PropÃ³sito |
|------------|------------|-----------|
| **Fonte de Dados** | Kaggle API | Download de datasets pÃºblicos |
| **Processamento** | Python + Pandas | ETL e detecÃ§Ã£o de CDC |
| **Formato** | Parquet (Snappy) | Armazenamento colunar otimizado |
| **Storage** | AWS S3 | Data Lake (camada RAW) |
| **OrquestraÃ§Ã£o** | Python Schedule | ExecuÃ§Ãµes automatizadas |

---

## ğŸš€ Quick Start

### 1. ConfiguraÃ§Ã£o

```bash
# Clone e instale
git clone https://github.com/TeoMeWhy/cdc-kaggle.git
cd cdc-kaggle
pip install -r requirements.txt

# Configure credenciais (.env)
KAGGLE_USERNAME=seu_usuario
KAGGLE_KEY=sua_api_key
AWS_ACCESS_KEY_ID=sua_access_key
AWS_SECRET_ACCESS_KEY=sua_secret_key
```

### 2. ExecuÃ§Ã£o

```bash
# ExecuÃ§Ã£o Ãºnica
python main.py

# Modo agendado (loop contÃ­nuo)
python main.py --schedule
```

### 3. Resultado

```
s3://seu-bucket/prefix/
â”œâ”€â”€ full-load/
â”‚   â”œâ”€â”€ clientes.parquet
â”‚   â”œâ”€â”€ produtos.parquet
â”‚   â””â”€â”€ transacoes.parquet
â””â”€â”€ cdc/
    â”œâ”€â”€ clientes_20251004_120000.parquet
    â”œâ”€â”€ produtos_20251004_120001.parquet
    â””â”€â”€ transacoes_20251004_120002.parquet
```

---

## ğŸ“Š Change Data Capture (CDC)

### OperaÃ§Ãµes Detectadas

| OperaÃ§Ã£o | CÃ³digo | LÃ³gica |
|----------|--------|--------|
| **INSERT** | `I` | Registros presentes apenas no snapshot atual |
| **UPDATE** | `U` | Registros com PK igual mas timestamp maior |
| **DELETE** | `D` | Registros presentes apenas no snapshot anterior |

### Estrutura do Arquivo CDC

```python
# Exemplo: clientes_20251004_120000.parquet
{
    "idCliente": 12345,
    "Nome": "JoÃ£o Silva",
    "Email": "joao@email.com",
    "DtAtualizacao": "2025-10-04 12:00:00",
    "_cdc_operation": "U",          # I, U ou D
    "_cdc_timestamp": "2025-10-04 12:00:00"
}
```

---

## ğŸ“ˆ BenefÃ­cios TÃ©cnicos

### Performance
- âœ… **ReduÃ§Ã£o de 70%** no tamanho dos arquivos (Parquet vs CSV)
- âœ… **CompressÃ£o Snappy**: balanÃ§o ideal entre velocidade e espaÃ§o
- âœ… **Processamento incremental**: apenas mudanÃ§as sÃ£o transferidas

### Escalabilidade
- âœ… Suporte a mÃºltiplas tabelas via configuraÃ§Ã£o JSON
- âœ… Retry logic com exponential backoff
- âœ… Logging estruturado para troubleshooting

### IntegraÃ§Ã£o
- âœ… CompatÃ­vel com Databricks/Spark (Parquet)
- âœ… Pronto para Delta Lake UPSERT operations
- âœ… Metadados CDC facilitam merge operations

---

## ğŸ“ DocumentaÃ§Ã£o TÃ©cnica

- ğŸ“˜ [**Setup Completo**](docs/SETUP.md) - InstalaÃ§Ã£o e configuraÃ§Ã£o detalhada
- ğŸ—ï¸ [**Arquitetura**](docs/ARCHITECTURE.md) - Design tÃ©cnico e decisÃµes arquiteturais
- â° [**Agendamento**](docs/AGENDAMENTO.md) - AutomaÃ§Ã£o via Task Scheduler

---

## ğŸ”„ Roadmap

### âœ… Implementado (Parte 1)
- [x] Download automatizado Kaggle
- [x] DetecÃ§Ã£o de CDC (Insert/Update/Delete)
- [x] ConversÃ£o para Parquet otimizado
- [x] Upload para S3 (full-load + CDC)
- [x] Logging e tratamento de erros

### ğŸš§ Em Andamento (Parte 2)
- [ ] Lakehouse com Delta Lake ([ver projeto](https://github.com/tmarsbr/meu-lago-mago))
- [ ] Streaming com Structured Streaming
- [ ] UPSERT operations em BRONZE
- [ ] Change Data Feed (CDF) para SILVER
- [ ] Cubos OLAP em GOLD

---

## ğŸ‘¨â€ğŸ’» Autor

**Tiago Mendes**
- GitHub: [@TeoMeWhy](https://github.com/TeoMeWhy)
- LinkedIn: [Tiago Mendes](https://linkedin.com/in/seu-perfil)
- Kaggle: [@teocalvo](https://www.kaggle.com/teocalvo)

---

## ğŸ“„ LicenÃ§a

MIT License - veja [LICENSE](LICENSE) para detalhes.

---

<div align="center">

**â­ Se este projeto agregou valor, considere deixar uma estrela**

**Parte 1/2** | [Veja a Parte 2 â†’](https://github.com/tmarsbr/meu-lago-mago)

</div>
