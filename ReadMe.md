# Sistema de CDC (Change Data Capture) - Kaggle para S3# CDC com dados do Kaggle



Sistema completo de **Change Data Capture** que automatiza o download de datasets do Kaggle, processa snapshots em formato Parquet e envia para um data lake no AWS S3.## Sobre



## ğŸ“‹ CaracterÃ­sticasAutomaÃ§Ã£o para baixar dados do Kaggle e criar arquivos de CDC (Change Data Capture) comparando as duas Ãºltimas versÃµes.



- âœ… Download automatizado de datasets do KaggleOs arquivos de CDC estarÃ£o disponÃ­veis no diretÃ³rio `./data/cdc/` no seguinte formato: `{tabela}_YYYMMDD_HHMMSS.csv`

- âœ… ConversÃ£o de CSV para Parquet

- âœ… Upload de full-load para S3## Setup

- âœ… DetecÃ§Ã£o de CDC (inserÃ§Ãµes, atualizaÃ§Ãµes e deleÃ§Ãµes)

- âœ… Logging estruturado com timestamps### Instalando dependÃªncias

- âœ… Tratamento robusto de erros

- âœ… Suporte a execuÃ§Ã£o Ãºnica ou agendadaSugirimos criar um ambiente virtual apropriado para execuÃ§Ã£o do programa. ApÃ³s isso, pode instalar as depedÃªncias usando `pip`:

- âœ… ConfiguraÃ§Ã£o via JSON e variÃ¡veis de ambiente

```bash

## ğŸ› ï¸ Requisitospip install -r requirements.txt

```

- Python 3.8+

- Conta Kaggle com credenciais de APIPara garantir que a execuÃ§Ã£o funciona corretamente, as seguintes variÃ¡veis ambientes devem estar configuradas:

- Credenciais AWS com permissÃµes no S3

- Bucket S3 criado```bash

KAGGLE_USERNAME=

## ğŸ“¦ InstalaÃ§Ã£oKAGGLE_KEY=

```

1. Clone o repositÃ³rio:

```bashCaso tenha interesse em alterar a periodicidade de atualizaÃ§Ã£o da carga, altere o valor da variÃ¡vel `timer` em `config.json`.

git clone https://github.com/TeoMeWhy/cdc-kaggle.git

cd cdc-kaggle## ExecuÃ§Ã£o

```

```bash

2. Instale as dependÃªncias:python main.py

```bash```
pip install -r requirements.txt
```

3. Configure as credenciais no arquivo `.env`:
```env
# Credenciais Kaggle
KAGGLE_USERNAME=seu_usuario
KAGGLE_KEY=sua_chave_api

# Credenciais AWS
AWS_ACCESS_KEY_ID=sua_access_key
AWS_SECRET_ACCESS_KEY=sua_secret_key
AWS_REGION=us-east-1
```

4. Configure o arquivo `config.json` com suas preferÃªncias:
```json
{
    "dataset_name": "teocalvo/teomewhy-loyalty-system",
    "aws": {
        "bucket": "meudatalake-raw",
        "prefix": "upcell",
        "region": "us-east-1"
    },
    "timer": {
        "unit": "hours",
        "value": 6
    },
    "tables": [...]
}
```

## ğŸš€ Uso

### Modo Agendado (Loop ContÃ­nuo)

Executa o pipeline continuamente com o intervalo configurado:

```bash
python main.py
```

### Modo ExecuÃ§Ã£o Ãšnica

Executa o pipeline apenas uma vez:

```bash
python main.py --once
```

### Pular Download (Para Testes)

Executa o pipeline usando dados jÃ¡ baixados localmente:

```bash
python main.py --once --skip-download
```

### Usar Arquivo de ConfiguraÃ§Ã£o Customizado

```bash
python main.py --config minha_config.json
```

## ğŸ“ Estrutura de DiretÃ³rios

```
cdc-kaggle/
â”œâ”€â”€ main.py              # Script principal
â”œâ”€â”€ config.json          # ConfiguraÃ§Ãµes
â”œâ”€â”€ .env                 # Credenciais (nÃ£o commitar!)
â”œâ”€â”€ requirements.txt     # DependÃªncias Python
â”œâ”€â”€ cdc_pipeline.log     # Log de execuÃ§Ã£o
â””â”€â”€ data/
    â”œâ”€â”€ actual/          # Snapshot atual (apÃ³s download)
    â”œâ”€â”€ last/            # Snapshot anterior (referÃªncia)
    â””â”€â”€ cdc/             # Arquivos CDC gerados
```

## ğŸ—‚ï¸ Estrutura no S3

Os arquivos sÃ£o organizados da seguinte forma:

```
s3://meudatalake-raw/upcell/
â”œâ”€â”€ full-load/
â”‚   â”œâ”€â”€ clientes/
â”‚   â”‚   â””â”€â”€ clientes.parquet
â”‚   â”œâ”€â”€ produtos/
â”‚   â”‚   â””â”€â”€ produtos.parquet
â”‚   â”œâ”€â”€ transacoes/
â”‚   â”‚   â””â”€â”€ transacoes.parquet
â”‚   â””â”€â”€ transacao_produto/
â”‚       â””â”€â”€ transacao_produto.parquet
â””â”€â”€ cdc/
    â”œâ”€â”€ clientes/
    â”‚   â””â”€â”€ clientes_20251002_143025.parquet
    â”œâ”€â”€ produtos/
    â”‚   â””â”€â”€ produtos_20251002_143025.parquet
    â”œâ”€â”€ transacoes/
    â”‚   â””â”€â”€ transacoes_20251002_143025.parquet
    â””â”€â”€ transacao_produto/
        â””â”€â”€ transacao_produto_20251002_143025.parquet
```

## ğŸ”„ Fluxo do Pipeline

1. **Download**: Baixa o dataset do Kaggle e descompacta em `./data/actual/`
2. **Full-load**: 
   - LÃª arquivos CSV
   - Converte para Parquet
   - Faz upload para `s3://bucket/prefix/full-load/`
3. **CDC**:
   - Compara snapshot atual com anterior
   - Identifica inserÃ§Ãµes (op='I'), atualizaÃ§Ãµes (op='U') e deleÃ§Ãµes (op='D')
   - Salva em Parquet com timestamp
   - Faz upload para `s3://bucket/prefix/cdc/`
4. **PÃ³s-processamento**: Move arquivos de `actual/` para `last/` para prÃ³ximo ciclo

## ğŸ“Š OperaÃ§Ãµes de CDC

| OperaÃ§Ã£o | CÃ³digo | DescriÃ§Ã£o |
|----------|--------|-----------|
| InserÃ§Ã£o | `I` | Registros presentes no snapshot atual mas nÃ£o no anterior |
| AtualizaÃ§Ã£o | `U` | Registros com PK igual e campo de data maior no snapshot atual |
| DeleÃ§Ã£o | `D` | Registros presentes no snapshot anterior mas nÃ£o no atual |

## ğŸ”§ ConfiguraÃ§Ã£o das Tabelas

No `config.json`, configure cada tabela com:

- `name`: Nome da tabela (e do arquivo CSV)
- `sep`: Separador usado no CSV (ex: `;`, `,`)
- `pk`: Nome da coluna de chave primÃ¡ria
- `date_field`: Campo usado para detectar atualizaÃ§Ãµes

Exemplo:

```json
{
    "sep": ";",
    "name": "clientes",
    "date_field": "DtAtualizacao",
    "pk": "IdCliente"
}
```

## ğŸ“ Logs

Os logs sÃ£o salvos em dois locais:
- **Console** (stdout): Logs em tempo real
- **Arquivo** (`cdc_pipeline.log`): HistÃ³rico completo com encoding UTF-8

NÃ­veis de log:
- `INFO`: OperaÃ§Ãµes principais
- `WARNING`: Avisos (ex: arquivo nÃ£o encontrado)
- `ERROR`: Erros com traceback completo
- `DEBUG`: InformaÃ§Ãµes detalhadas (ativÃ¡vel alterando `logging.INFO` para `logging.DEBUG`)

## ğŸ›¡ï¸ Tratamento de Erros

O sistema possui tratamento robusto de erros:

- **Download falha**: Pipeline interrompido, erro registrado
- **Upload S3 falha**: Erro registrado, continua com prÃ³ximas tabelas
- **Snapshot anterior ausente**: Considera todas as linhas como inserÃ§Ãµes
- **Erro em iteraÃ§Ã£o agendada**: Aguarda prÃ³ximo ciclo automaticamente

## ğŸ” SeguranÃ§a

âš ï¸ **IMPORTANTE**: Nunca commite o arquivo `.env` com credenciais!

Adicione ao `.gitignore`:
```
.env
*.log
data/
```

## ğŸ“– Exemplos de Uso

### Executar uma vez e verificar logs
```bash
python main.py --once
tail -f cdc_pipeline.log
```

### Testar com dados locais
```bash
python main.py --once --skip-download
```

### Executar em background (Linux/Mac)
```bash
nohup python main.py > output.log 2>&1 &
```

### Executar em background (Windows PowerShell)
```powershell
Start-Process python -ArgumentList "main.py" -WindowStyle Hidden
```

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para:

1. Fazer fork do projeto
2. Criar uma branch para sua feature (`git checkout -b feature/MinhaFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona MinhaFeature'`)
4. Push para a branch (`git push origin feature/MinhaFeature`)
5. Abrir um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ‘¨â€ğŸ’» Autor

**TeoMeWhy**
- GitHub: [@TeoMeWhy](https://github.com/TeoMeWhy)
- Kaggle: [teocalvo](https://www.kaggle.com/teocalvo)

## ğŸ™ Agradecimentos

- Kaggle pela API de datasets
- AWS pela infraestrutura S3
- Comunidade Python por bibliotecas incrÃ­veis

---

**Desenvolvido com â¤ï¸ e GitHub Copilot**
