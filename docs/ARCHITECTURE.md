# ğŸ—ï¸ Arquitetura do Sistema CDC

Este documento detalha a arquitetura tÃ©cnica do Sistema de Change Data Capture (CDC).

---

## ğŸ“ VisÃ£o Geral da Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SISTEMA CDC PIPELINE                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SOURCE     â”‚       â”‚  PROCESSING  â”‚       â”‚ DESTINATION  â”‚
â”‚   (Kaggle)   â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚   (Python)   â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚   (AWS S3)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”œâ”€â”€ Extract (Kaggle API)
                              â”œâ”€â”€ Transform (Pandas)
                              â”œâ”€â”€ CDC Detection (Diff)
                              â””â”€â”€ Load (Boto3)
```

---

## ğŸ”„ Fluxo de Dados Detalhado

### 1. ExtraÃ§Ã£o (Extract)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kaggle API  â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â”œâ”€ Dataset Download (ZIP)
      â”œâ”€ Extraction (.csv files)
      â””â”€ Storage (data/actual/)
```

**Tecnologias:**
- `kaggle` library
- Python `zipfile`
- AutenticaÃ§Ã£o via `kaggle.json`

**Processo:**
1. AutenticaÃ§Ã£o via API Token
2. Download do dataset completo (formato ZIP)
3. ExtraÃ§Ã£o dos arquivos CSV
4. Armazenamento em `data/actual/`

### 2. TransformaÃ§Ã£o (Transform)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      TRANSFORMATION LAYER            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. CSV to Parquet Conversion       â”‚
â”‚  2. Data Type Optimization           â”‚
â”‚  3. Compression (Snappy)             â”‚
â”‚  4. Schema Validation                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tecnologias:**
- `pandas` (DataFrames)
- `pyarrow` (Parquet engine)

**OtimizaÃ§Ãµes:**
- ConversÃ£o de tipos de dados
- CompressÃ£o Snappy (balanÃ§o velocidade/tamanho)
- ReduÃ§Ã£o de ~70% no tamanho dos arquivos

### 3. CDC Detection

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LAST      â”‚         â”‚   ACTUAL    â”‚
â”‚  Snapshot   â”‚         â”‚  Snapshot   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  CDC DETECTOR   â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚  â€¢ INSERTS      â”‚
     â”‚  â€¢ UPDATES      â”‚
     â”‚  â€¢ DELETES      â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  CDC Parquet    â”‚
     â”‚  + Metadata     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Algoritmo de DetecÃ§Ã£o:**

```python
# Pseudo-cÃ³digo
def detect_cdc(last_df, actual_df, pk, date_field):
    # 1. INSERTS: Registros em actual que nÃ£o estÃ£o em last
    inserts = actual_df[~actual_df[pk].isin(last_df[pk])]
    
    # 2. DELETES: Registros em last que nÃ£o estÃ£o em actual
    deletes = last_df[~last_df[pk].isin(actual_df[pk])]
    
    # 3. UPDATES: Registros com mesmo PK mas dados diferentes
    merged = actual_df.merge(last_df, on=pk, suffixes=('_new', '_old'))
    if date_field:
        updates = merged[merged[f'{date_field}_new'] > merged[f'{date_field}_old']]
    else:
        updates = merged[merged != merged]  # Compare all fields
    
    return inserts, updates, deletes
```

**Metadados do CDC:**

Cada arquivo CDC contÃ©m:
- `_cdc_operation`: `I` (Insert), `U` (Update), `D` (Delete)
- `_cdc_timestamp`: Timestamp da geraÃ§Ã£o do CDC
- Todos os campos originais da tabela

### 4. Carga (Load)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         AWS S3 STRUCTURE             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  bucket/                             â”‚
â”‚  â””â”€â”€ prefix/                         â”‚
â”‚      â”œâ”€â”€ full-load/                  â”‚
â”‚      â”‚   â”œâ”€â”€ clientes.parquet        â”‚
â”‚      â”‚   â”œâ”€â”€ produtos.parquet        â”‚
â”‚      â”‚   â””â”€â”€ transacoes.parquet      â”‚
â”‚      â””â”€â”€ cdc/                        â”‚
â”‚          â”œâ”€â”€ clientes_YYYYMMDD_HHMMSS.parquet
â”‚          â”œâ”€â”€ produtos_YYYYMMDD_HHMMSS.parquet
â”‚          â””â”€â”€ transacoes_YYYYMMDD_HHMMSS.parquet
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tecnologias:**
- `boto3` (AWS SDK)
- S3 Multipart Upload
- Retry logic com exponential backoff

---

## ğŸ—„ï¸ Modelo de Dados

### Estrutura Local

```
data/
â”œâ”€â”€ actual/              # Dados atuais (apÃ³s download)
â”‚   â”œâ”€â”€ clientes.csv
â”‚   â”œâ”€â”€ produtos.csv
â”‚   â””â”€â”€ transacoes.csv
â”‚
â”œâ”€â”€ last/                # Snapshot anterior (para CDC)
â”‚   â”œâ”€â”€ clientes.csv
â”‚   â”œâ”€â”€ produtos.csv
â”‚   â””â”€â”€ transacoes.csv
â”‚
â””â”€â”€ cdc/                 # CDC gerado (Parquet)
    â”œâ”€â”€ clientes_20251004_095645.parquet
    â”œâ”€â”€ produtos_20251004_095646.parquet
    â””â”€â”€ transacoes_20251004_095647.parquet
```

### Schema CDC (Parquet)

```
clientes_CDC.parquet
â”œâ”€â”€ idCliente (int64)              # Primary Key
â”œâ”€â”€ Nome (string)
â”œâ”€â”€ Email (string)
â”œâ”€â”€ DtAtualizacao (datetime64)     # Date field
â”œâ”€â”€ _cdc_operation (string)        # I/U/D
â””â”€â”€ _cdc_timestamp (datetime64)    # Timestamp do CDC
```

---

## âš™ï¸ Componentes do Sistema

### 1. Configuration Manager

```python
class ConfigManager:
    - load_config(path: str) -> Dict
    - validate_config(config: Dict) -> bool
    - get_table_config(table_name: str) -> Dict
```

**Responsabilidades:**
- Carregamento do `config.json`
- ValidaÃ§Ã£o de parÃ¢metros
- Acesso centralizado a configuraÃ§Ãµes

### 2. Kaggle Downloader

```python
class KaggleDownloader:
    - authenticate() -> bool
    - download_dataset(dataset_name: str) -> bool
    - extract_files(zip_path: str, dest: str) -> List[str]
```

**Responsabilidades:**
- AutenticaÃ§Ã£o na API
- Download de datasets
- ExtraÃ§Ã£o de arquivos

### 3. CDC Engine

```python
class CDCEngine:
    - detect_changes(last_df, actual_df, pk, date_field) -> DataFrame
    - classify_operations(changes: DataFrame) -> DataFrame
    - add_metadata(df: DataFrame) -> DataFrame
```

**Responsabilidades:**
- DetecÃ§Ã£o de mudanÃ§as
- ClassificaÃ§Ã£o de operaÃ§Ãµes (I/U/D)
- AdiÃ§Ã£o de metadados CDC

### 4. S3 Uploader

```python
class S3Uploader:
    - get_client() -> boto3.client
    - upload_file(local_path, s3_path) -> bool
    - upload_full_load(df, table_name) -> bool
    - upload_cdc(df, table_name, timestamp) -> bool
```

**Responsabilidades:**
- ConexÃ£o com AWS S3
- Upload de arquivos
- Gerenciamento de paths no S3

### 5. Pipeline Orchestrator

```python
class PipelineOrchestrator:
    - run_full_load() -> bool
    - run_cdc() -> bool
    - schedule_execution(interval: int, unit: str)
    - cleanup_old_files(keep_n: int)
```

**Responsabilidades:**
- OrquestraÃ§Ã£o do fluxo completo
- Agendamento de execuÃ§Ãµes
- Limpeza de arquivos antigos

---

## ğŸ” SeguranÃ§a

### AutenticaÃ§Ã£o e AutorizaÃ§Ã£o

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      CREDENTIALS MANAGEMENT         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Kaggle API Token                   â”‚
â”‚  â”œâ”€â”€ ~/.kaggle/kaggle.json          â”‚
â”‚  â””â”€â”€ Env Vars: KAGGLE_USERNAME/KEY  â”‚
â”‚                                      â”‚
â”‚  AWS Credentials                     â”‚
â”‚  â”œâ”€â”€ ~/.aws/credentials             â”‚
â”‚  â”œâ”€â”€ Env Vars: AWS_ACCESS_KEY_ID    â”‚
â”‚  â””â”€â”€ IAM Role (recommended)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Boas PrÃ¡ticas:**
- Credenciais em `.env` (nÃ£o versionado)
- IAM Roles em produÃ§Ã£o
- Least Privilege principle
- Credential rotation

### PermissÃµes AWS IAM

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:GetObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::seu-bucket/*",
        "arn:aws:s3:::seu-bucket"
      ]
    }
  ]
}
```

---

## ğŸ“Š Logging e Monitoramento

### NÃ­veis de Log

```python
logging.DEBUG    # Detalhes tÃ©cnicos, debug
logging.INFO     # InformaÃ§Ãµes gerais (padrÃ£o)
logging.WARNING  # Avisos nÃ£o crÃ­ticos
logging.ERROR    # Erros que impedem operaÃ§Ãµes
logging.CRITICAL # Erros crÃ­ticos do sistema
```

### Estrutura de Logs

```
2025-10-04 09:56:43 - CDC_PIPELINE - INFO - Iniciando pipeline CDC
2025-10-04 09:56:45 - KAGGLE_DL - INFO - Dataset baixado: 125MB
2025-10-04 09:56:47 - CDC_ENGINE - INFO - CDC detectado: 15 inserts, 3 updates
2025-10-04 09:56:50 - S3_UPLOAD - INFO - Upload concluÃ­do: s3://bucket/prefix/cdc/
```

**Destinos:**
- Console (stdout)
- Arquivo: `cdc_pipeline.log`
- (Futuro) CloudWatch Logs

---

## ğŸš€ Escalabilidade

### EstratÃ©gias de Escala

**Horizontal Scaling:**
- Processar mÃºltiplas tabelas em paralelo
- Usar multiprocessing/threading

**Vertical Scaling:**
- Otimizar uso de memÃ³ria com chunks
- Processar DataFrames em batches

**Cloud Scaling:**
- Migrar para AWS Lambda (serverless)
- Usar AWS Glue para ETL gerenciado
- Implementar com Apache Airflow

### Performance

**Benchmarks:**
- 1GB CSV â†’ 300MB Parquet (~70% reduÃ§Ã£o)
- 1M registros processados em ~30s
- Upload S3: ~10MB/s (dependente de rede)

---

## ğŸ”§ Tratamento de Erros

### EstratÃ©gias de Retry

```python
@retry(max_attempts=3, backoff=2)
def upload_to_s3(file_path, s3_path):
    try:
        s3_client.upload_file(file_path, bucket, s3_path)
    except ClientError as e:
        logger.error(f"Erro no upload: {e}")
        raise
```

**CenÃ¡rios:**
- Network timeouts
- S3 throttling
- Kaggle API rate limits
- Disk space issues

---

## ğŸ“ˆ Melhorias Futuras

### Roadmap TÃ©cnico

**Fase 1: Estabilidade**
- âœ… Pipeline bÃ¡sico funcional
- âœ… Logging estruturado
- âœ… Tratamento de erros
- [ ] Testes unitÃ¡rios
- [ ] Testes de integraÃ§Ã£o

**Fase 2: OtimizaÃ§Ã£o**
- [ ] Processamento paralelo
- [ ] Chunked processing
- [ ] Cache de metadata
- [ ] Compression tuning

**Fase 3: EvoluÃ§Ã£o**
- [ ] Suporte a Delta Lake
- [ ] IntegraÃ§Ã£o com Airflow
- [ ] Monitoring dashboard
- [ ] Data quality checks

---

## ğŸ“š ReferÃªncias

- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [AWS Boto3 Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
- [Apache Parquet Format](https://parquet.apache.org/docs/)
- [Kaggle API Documentation](https://github.com/Kaggle/kaggle-api)
- [CDC Best Practices](https://www.confluent.io/learn/change-data-capture/)

---

**Ãšltima atualizaÃ§Ã£o:** Outubro 2025
